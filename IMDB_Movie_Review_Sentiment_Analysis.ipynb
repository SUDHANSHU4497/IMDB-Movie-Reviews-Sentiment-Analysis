{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SUDHANSHU4497/IMDB-Movie-Reviews-Sentiment-Analysis/blob/main/IMDB_Movie_Review_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Keras-Preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2gHunapbSmf",
        "outputId": "2dd17320-6783-4324-e377-b6cc3f6525c6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Keras-Preprocessing in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing) (1.26.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import models\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Embedding, LSTM # Import LSTM directly from keras.layers\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import collections\n",
        "import re\n",
        "import string"
      ],
      "metadata": {
        "id": "DEkDUlyybCab"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHnPVIv6cc9a",
        "outputId": "e313e826-808d-47a2-dd3e-466ac5c913a7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0rSgOwWaerz"
      },
      "source": [
        "### Read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HfR90IH1aer0"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/dataset/imdb_labelled.tsv', header = None, delimiter='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZEDLK4waer0"
      },
      "outputs": [],
      "source": [
        "data.columns = ['Text', 'Label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-dC71fpaer0",
        "outputId": "7914bc39-bf6c-4fd6-a00b-f01ea1110788"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not sure who was more lost - the flat characte...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Very little music or anything to speak of.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The best scene in the movie was when Gerardo i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  Label\n",
              "0  A very, very, very slow-moving, aimless movie ...      0\n",
              "1  Not sure who was more lost - the flat characte...      0\n",
              "2  Attempting artiness with black & white and cle...      0\n",
              "3       Very little music or anything to speak of.        0\n",
              "4  The best scene in the movie was when Gerardo i...      1"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wzg9nXueaer0",
        "outputId": "841136e8-f1ca-48e1-9a2e-cf3b6ea0337d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.Label.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIiDFRO5aer0",
        "outputId": "53954393-a468-4501-b422-464db249ce20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(748, 2)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7YwmJlUaer1"
      },
      "outputs": [],
      "source": [
        "pos = []\n",
        "neg = []\n",
        "for l in data.Label:\n",
        "    if l == 0:\n",
        "        pos.append(0)\n",
        "        neg.append(1)\n",
        "    elif l == 1:\n",
        "        pos.append(1)\n",
        "        neg.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un9HjsXkaer1"
      },
      "outputs": [],
      "source": [
        "data['Pos']= pos\n",
        "data['Neg']= neg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNQQy3zVaer1",
        "outputId": "b3e6c162-a946-476d-b38f-2d608569f4e1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Label</th>\n",
              "      <th>Pos</th>\n",
              "      <th>Neg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not sure who was more lost - the flat characte...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Very little music or anything to speak of.</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The best scene in the movie was when Gerardo i...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  Label  Pos  Neg\n",
              "0  A very, very, very slow-moving, aimless movie ...      0    0    1\n",
              "1  Not sure who was more lost - the flat characte...      0    0    1\n",
              "2  Attempting artiness with black & white and cle...      0    0    1\n",
              "3       Very little music or anything to speak of.        0    0    1\n",
              "4  The best scene in the movie was when Gerardo i...      1    1    0"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BES1-Mggaer1"
      },
      "source": [
        "### Clean data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8NyJ0Ujaer1"
      },
      "outputs": [],
      "source": [
        "def remove_punct(text):\n",
        "    text_nopunct = ''\n",
        "    text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
        "    return text_nopunct\n",
        "\n",
        "data['Text_Clean'] = data['Text'].apply(lambda x: remove_punct(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHadTUATaer2"
      },
      "outputs": [],
      "source": [
        "from nltk import word_tokenize, WordNetLemmatizer\n",
        "tokens = [word_tokenize(sen) for sen in data.Text_Clean]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrIZ0CmOaer2"
      },
      "outputs": [],
      "source": [
        "def lower_token(tokens):\n",
        "    return [w.lower() for w in tokens]\n",
        "\n",
        "lower_tokens = [lower_token(token) for token in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_okNHM9aer2"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stoplist = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6fYX6Ixaer2"
      },
      "outputs": [],
      "source": [
        "def remove_stop_words(tokens):\n",
        "    return [word for word in tokens if word not in stoplist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcXw_J4uaer2"
      },
      "outputs": [],
      "source": [
        "filtered_words = [remove_stop_words(sen) for sen in lower_tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ehy_sraHaer2"
      },
      "outputs": [],
      "source": [
        "result = [' '.join(sen) for sen in filtered_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9UMQH6iaer2"
      },
      "outputs": [],
      "source": [
        "data['Text_Final'] = result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4L3SlSVaer3"
      },
      "outputs": [],
      "source": [
        "data['tokens'] = filtered_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZaIDCKjaer3"
      },
      "outputs": [],
      "source": [
        "data = data[['Text_Final', 'tokens', 'Label', 'Pos', 'Neg']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayFJrFMVaer3",
        "outputId": "7b93bfcb-f6ac-4db0-924d-78729b8432ea"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text_Final</th>\n",
              "      <th>tokens</th>\n",
              "      <th>Label</th>\n",
              "      <th>Pos</th>\n",
              "      <th>Neg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>slowmoving aimless movie distressed drifting y...</td>\n",
              "      <td>[slowmoving, aimless, movie, distressed, drift...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sure lost flat characters audience nearly half...</td>\n",
              "      <td>[sure, lost, flat, characters, audience, nearl...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>attempting artiness black white clever camera ...</td>\n",
              "      <td>[attempting, artiness, black, white, clever, c...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>little music anything speak</td>\n",
              "      <td>[little, music, anything, speak]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          Text_Final  \\\n",
              "0  slowmoving aimless movie distressed drifting y...   \n",
              "1  sure lost flat characters audience nearly half...   \n",
              "2  attempting artiness black white clever camera ...   \n",
              "3                        little music anything speak   \n",
              "\n",
              "                                              tokens  Label  Pos  Neg  \n",
              "0  [slowmoving, aimless, movie, distressed, drift...      0    0    1  \n",
              "1  [sure, lost, flat, characters, audience, nearl...      0    0    1  \n",
              "2  [attempting, artiness, black, white, clever, c...      0    0    1  \n",
              "3                   [little, music, anything, speak]      0    0    1  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[:4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBB06SE5aer3"
      },
      "source": [
        "### Split data into test and train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elUy81BDaer3"
      },
      "outputs": [],
      "source": [
        "data_train, data_test = train_test_split(data, test_size=0.10, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ctS1f3Eaer3",
        "outputId": "dd2d90ae-c6b8-4660-e4e5-24725fc9cd79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7218 words total, with a vocabulary size of 2881\n",
            "Max sentence length is 789\n"
          ]
        }
      ],
      "source": [
        "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
        "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
        "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
        "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvKn6pLdaer4",
        "outputId": "2c5392bf-6743-4699-c0c8-eefb7311d812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "580 words total, with a vocabulary size of 457\n",
            "Max sentence length is 24\n"
          ]
        }
      ],
      "source": [
        "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
        "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
        "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
        "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLH7Jrm_aer4"
      },
      "source": [
        "### Load Google News Word2Vec model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32denrHQaer4",
        "outputId": "3d52ab83-d888-4927-e227-b61e7183b863"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/saadarshad/anaconda3/envs/amna_bot/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ]
        }
      ],
      "source": [
        "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMzlXgadaer4"
      },
      "outputs": [],
      "source": [
        "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
        "    if len(tokens_list)<1:\n",
        "        return np.zeros(k)\n",
        "    if generate_missing:\n",
        "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
        "    else:\n",
        "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
        "    length = len(vectorized)\n",
        "    summed = np.sum(vectorized, axis=0)\n",
        "    averaged = np.divide(summed, length)\n",
        "    return averaged\n",
        "\n",
        "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
        "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors,\n",
        "                                                                                generate_missing=generate_missing))\n",
        "    return list(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF6WoVQRaer5"
      },
      "source": [
        "### Get Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-xywU5aaer5"
      },
      "outputs": [],
      "source": [
        "training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCOvmTLGaer5"
      },
      "outputs": [],
      "source": [
        "MAX_SEQUENCE_LENGTH = 50\n",
        "EMBEDDING_DIM = 300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03EVg2Kyaer5"
      },
      "source": [
        "### Tokenize and Pad sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSIo9ppMaer5",
        "outputId": "cbd55b0c-c033-4f62-9211-2a7225b07240"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2881 unique tokens.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(data_train[\"Text_Final\"].tolist())\n",
        "training_sequences = tokenizer.texts_to_sequences(data_train[\"Text_Final\"].tolist())\n",
        "\n",
        "train_word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(train_word_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLUgR6mEaer6"
      },
      "outputs": [],
      "source": [
        "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDqsx90oaer6",
        "outputId": "f0f6156e-66b0-421a-931b-9bffbc187085"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2882, 300)\n"
          ]
        }
      ],
      "source": [
        "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
        "for word,index in train_word_index.items():\n",
        "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
        "print(train_embedding_weights.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiH--16Gaer6"
      },
      "outputs": [],
      "source": [
        "test_sequences = tokenizer.texts_to_sequences(data_test[\"Text_Final\"].tolist())\n",
        "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzqNEfsfaer6"
      },
      "source": [
        "### Define RNN-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMOBYn26aer6"
      },
      "outputs": [],
      "source": [
        "label_names = ['Pos', 'Neg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITcUnfgpaesB"
      },
      "outputs": [],
      "source": [
        "y_train = data_train[label_names].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-EYIlqsaesB"
      },
      "outputs": [],
      "source": [
        "x_train = train_cnn_data\n",
        "y_tr = y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGEdnAPpaesB"
      },
      "outputs": [],
      "source": [
        "def recurrent_nn(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
        "\n",
        "    embedding_layer = Embedding(num_words,\n",
        "                            embedding_dim,\n",
        "                            weights=[embeddings],\n",
        "                            input_length=max_sequence_length,\n",
        "                            trainable=False)\n",
        "\n",
        "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "#     lstm = LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(embedded_sequences)\n",
        "    lstm = LSTM(256)(embedded_sequences)\n",
        "\n",
        "    x = Dense(128, activation='relu')(lstm)\n",
        "    x = Dropout(0.2)(x)\n",
        "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(sequence_input, preds)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['acc'])\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuc_TXNBaesB",
        "outputId": "fd2b93c7-eebf-4d57-fded-687d811c2913"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_13 (InputLayer)        (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "embedding_13 (Embedding)     (None, 50, 300)           864600    \n",
            "_________________________________________________________________\n",
            "lstm_19 (LSTM)               (None, 256)               570368    \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 1,468,122\n",
            "Trainable params: 603,522\n",
            "Non-trainable params: 864,600\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = recurrent_nn(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM,\n",
        "                len(list(label_names)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6K1Siu2aesC"
      },
      "source": [
        "### Train RNN-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJPt0s8TaesC"
      },
      "outputs": [],
      "source": [
        "num_epochs = 5\n",
        "batch_size = 34"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZB_oWuTaesC",
        "outputId": "2b070db6-16ea-4631-a31d-476d5f3a1bdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 605 samples, validate on 68 samples\n",
            "Epoch 1/5\n",
            "605/605 [==============================] - 5s 8ms/step - loss: 0.0847 - acc: 0.9752 - val_loss: 0.5045 - val_acc: 0.8676\n",
            "Epoch 2/5\n",
            "605/605 [==============================] - 5s 8ms/step - loss: 0.0697 - acc: 0.9769 - val_loss: 0.4728 - val_acc: 0.8529\n",
            "Epoch 3/5\n",
            "605/605 [==============================] - 5s 8ms/step - loss: 0.0325 - acc: 0.9884 - val_loss: 0.4687 - val_acc: 0.8382\n",
            "Epoch 4/5\n",
            "605/605 [==============================] - 5s 8ms/step - loss: 0.0223 - acc: 0.9917 - val_loss: 0.5192 - val_acc: 0.8456\n",
            "Epoch 5/5\n",
            "605/605 [==============================] - 5s 8ms/step - loss: 0.0144 - acc: 0.9950 - val_loss: 0.5782 - val_acc: 0.8382\n"
          ]
        }
      ],
      "source": [
        "hist = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITE-aocAaesC"
      },
      "source": [
        "### Test RNN-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQhV5e-7aesC",
        "outputId": "543d1fcb-6574-4ac6-905c-a20e3195548e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r",
            "75/75 [==============================] - 0s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVQRpCbvaesC"
      },
      "outputs": [],
      "source": [
        "labels = [1, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJYN9s2FaesC"
      },
      "outputs": [],
      "source": [
        "prediction_labels=[]\n",
        "for p in predictions:\n",
        "    prediction_labels.append(labels[np.argmax(p)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNtzF0wkaesC",
        "outputId": "e9a90810-5597-4409-c417-149065e3a9a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8"
            ]
          },
          "execution_count": 185,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(data_test.Label==prediction_labels)/len(prediction_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34x-p-bnaesD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}